{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/gpc5sh7j5k3cls_fv_dpzntc0000gn/T/ipykernel_8261/878308936.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n"
     ]
    }
   ],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n",
    "\n",
    "# Load the FAISS store from disk\n",
    "faiss_store = FAISS.load_local(\"../data/faiss_db_4000\", hf_embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "gen_cfg = GenerationConfig.from_pretrained(model_id)\n",
    "gen_cfg.max_new_tokens=512\n",
    "gen_cfg.temperature=0.0000001 # 0.0\n",
    "gen_cfg.return_full_text=True\n",
    "gen_cfg.do_sample=True\n",
    "gen_cfg.repetition_penalty=1.11\n",
    "\n",
    "# Build a text2text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=gen_cfg\n",
    ")\n",
    "\n",
    "# Wrap the pipeline with LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_str = \"\"\"You are a timeline-based Question Answering system. Answer the question below based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template_str, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_tlqa(question: str, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant chunks from the FAISS store,\n",
    "    formats them into a prompt for FlanT5, \n",
    "    and returns the generative answer.\n",
    "    \"\"\"\n",
    "    # Retrieve top-k documents\n",
    "    docs = faiss_store.similarity_search(question, k=k)\n",
    "\n",
    "    # Combine the retrieved texts into one context string\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    print(context)\n",
    "\n",
    "    # Format the prompt\n",
    "    final_prompt = prompt.format(context=context, question=question)\n",
    "\n",
    "    # Generate an answer using the LLM\n",
    "    response = llm(final_prompt)\n",
    "\n",
    "    # The pipeline returns a list of dictionaries or a string. \n",
    "    # If using LangChain wrapper, it might return a string directly.\n",
    "    # Adjust as necessary if your pipeline returns a different format.\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Political Leader ==\n",
      "Diego Borja is the Coordination Minister for Economic Policy of Ecuador, appointed on December 23, 2008. He is also National President of the \"Movimiento Poder Ciudadano\" (Citizen's Power Movement). He was elected a member of the Constitutional Assembly on September 30, 2007, as a candidate of the alliance between the social-democratic \"Izquierda Democrática\" (Party of the Democratic Left), the Poder Ciudadano Movement, and two other democratic socialist movements, \"Acuerdo Democrático\" and \"Nuevo Acuerdo Nacional\". He was elected as the first candidate in the Province of Pichincha, which includes the capital city of Quito.\n",
      "Before forming \"Movimiento Poder Ciudadano\", Borja was a national leader of \"Movimiento Nuevo País\" (New Country Movement), where he was an adviser to Presidential candidate Freddy Ehlers, as well as alternate representative from Ecuador to the Andean Parliament.\n",
      "Question: Who is Diego Borja?\n",
      "Top-k Context Used: 1\n",
      "Answer: (2008, 2009)\n"
     ]
    }
   ],
   "source": [
    "# Replace this with any TLQA-style question you have\n",
    "test_question = \"Who is Diego Borja?\"\n",
    "k_value = 1\n",
    "\n",
    "answer = answer_tlqa(test_question, k=k_value)\n",
    "print(\"Question:\", test_question)\n",
    "print(\"Top-k Context Used:\", k_value)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import wptools\n",
    "from langchain.docstore.document import Document\n",
    "import mwparserfromhell\n",
    "import re\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references_and_comments(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove <ref>...</ref> tags, HTML comments, and any leftover reference templates\n",
    "    or other wiki markup that isn't helpful for an LLM.\n",
    "    \"\"\"\n",
    "    # Remove <ref>...</ref> including multiline\n",
    "    text = re.sub(r\"<ref[^>]*>.*?</ref>\", \"\", text, flags=re.DOTALL)\n",
    "    # Remove HTML comments <!-- ... -->\n",
    "    text = re.sub(r\"<!--.*?-->\", \"\", text, flags=re.DOTALL)\n",
    "    # Optionally remove any leftover template references like {{refn|...}} etc.\n",
    "    text = re.sub(r\"\\{\\{[Rr]efn[^}]*\\}\\}\", \"\", text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "def parse_infobox(template) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a single Infobox template into a key-value dict of parameters.\n",
    "    References and comments are removed from values.\n",
    "    \"\"\"\n",
    "    infobox_data = {}\n",
    "    for param in template.params:\n",
    "        param_name = str(param.name).strip()\n",
    "        # Use strip_code to remove wiki markup, then remove references\n",
    "        param_value = param.value.strip_code(normalize=True, collapse=True)\n",
    "        param_value = remove_references_and_comments(param_value)\n",
    "        infobox_data[param_name] = param_value\n",
    "    return infobox_data\n",
    "\n",
    "def parse_table(table_node) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Parse a single <table> tag into a list-of-lists representation (like CSV rows).\n",
    "    We strip out references/comments. You can adapt this to produce CSV or JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instead of parsing str(table_node) (which can lead to extra HTML wrappers), \n",
    "    # parse the *contents* of the table_node. This is often a more direct approach.\n",
    "    table_wikitext = table_node.contents\n",
    "    parsed_table = mwparserfromhell.parse(table_wikitext)\n",
    "\n",
    "    rows_data = []\n",
    "    # Find all <tr> tags inside the table\n",
    "    tr_tags = parsed_table.filter_tags(matches=lambda n: n.tag == 'tr')\n",
    "    for tr in tr_tags:\n",
    "        # For each <tr>, we look for <td> or <th> cells.\n",
    "        # Use .contents to isolate what's inside that row.\n",
    "        row_parsed = mwparserfromhell.parse(tr.contents)\n",
    "        cells = row_parsed.filter_tags(matches=lambda n: n.tag in ['td', 'th'])\n",
    "\n",
    "        row_values = []\n",
    "        for cell in cells:\n",
    "            # Convert wiki markup inside each cell to plain text\n",
    "            cell_text = cell.contents.strip_code(normalize=True, collapse=True)\n",
    "            cell_text = remove_references_and_comments(cell_text)\n",
    "            row_values.append(cell_text)\n",
    "\n",
    "        if row_values:\n",
    "            rows_data.append(row_values)\n",
    "\n",
    "    return rows_data\n",
    "\n",
    "def parse_wikitext_for_content_infobox_tables(wikitext: str) -> Tuple[str, List[Dict], List[List[List[str]]]]:\n",
    "    \"\"\"\n",
    "    Uses mwparserfromhell to parse the raw wikitext and extract:\n",
    "      1) Plain article text (with most wiki markup removed)\n",
    "      2) Infoboxes (templates containing 'infobox'), cleaned into key-value pairs\n",
    "      3) Tables (actual <table> HTML tags), parsed into list-of-lists\n",
    "    Returns (textual_content, infobox_dicts, tables_list_of_lists).\n",
    "    \"\"\"\n",
    "    parsed = mwparserfromhell.parse(wikitext)\n",
    "\n",
    "    # 1) Plain text (references removed)\n",
    "    textual_content = parsed.strip_code(normalize=True, collapse=True)\n",
    "    textual_content = remove_references_and_comments(textual_content)\n",
    "\n",
    "    # 2) Extract infoboxes (templates containing \"infobox\" in the name)\n",
    "    infobox_dicts = []\n",
    "    for template in parsed.filter_templates():\n",
    "        if \"infobox\" in template.name.lower():\n",
    "            infobox_data = parse_infobox(template)\n",
    "            infobox_dicts.append(infobox_data)\n",
    "\n",
    "    # 3) Extract tables (<table> tags), parse them\n",
    "    table_nodes = parsed.filter_tags(matches=lambda node: node.tag == 'table')\n",
    "    tables_list = []\n",
    "    for table_node in table_nodes:\n",
    "        table_data = parse_table(table_node)\n",
    "        # Only append if we actually got any rows from it\n",
    "        if table_data:\n",
    "            tables_list.append(table_data)\n",
    "\n",
    "    return textual_content, infobox_dicts, tables_list\n",
    "\n",
    "def fetch_wikipedia_content(title: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Fetch the textual content (wikitext) from a Wikipedia page using wptools.\n",
    "    Parse it with mwparserfromhell and split it into three parts:\n",
    "      - textual content\n",
    "      - infoboxes\n",
    "      - tables\n",
    "    Returns a list of Documents, or an empty list if no content is found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wptools.page(title, lang=\"en\", silent=True)\n",
    "        page.get_parse(show=False)\n",
    "\n",
    "        # Retrieve the wikitext content\n",
    "        text = page.data.get(\"wikitext\")\n",
    "        \n",
    "        documents = []\n",
    "        if text:\n",
    "            article_text, infoboxes, tables = parse_wikitext_for_content_infobox_tables(text)\n",
    "\n",
    "            # (1) Main article text\n",
    "            if article_text.strip():\n",
    "                text_doc = Document(\n",
    "                    page_content=article_text.strip(),\n",
    "                    metadata={\"title\": title, \"type\": \"text\"}\n",
    "                )\n",
    "                documents.append(text_doc)\n",
    "\n",
    "            # (2) Infoboxes\n",
    "            if infoboxes:\n",
    "                infobox_strings = []\n",
    "                for idx, ibox in enumerate(infoboxes, 1):\n",
    "                    ibox_str = f\"Infobox #{idx}:\\n\"\n",
    "                    for k, v in ibox.items():\n",
    "                        ibox_str += f\"  {k}: {v}\\n\"\n",
    "                    infobox_strings.append(ibox_str)\n",
    "                \n",
    "                infobox_doc = Document(\n",
    "                    page_content=\"\\n\\n\".join(infobox_strings),\n",
    "                    metadata={\"title\": title, \"type\": \"infoboxes\"}\n",
    "                )\n",
    "                documents.append(infobox_doc)\n",
    "\n",
    "            # (3) Tables\n",
    "            if tables:\n",
    "                table_strings = []\n",
    "                for idx, table_data in enumerate(tables, 1):\n",
    "                    table_str = f\"Table #{idx}\\n\"\n",
    "                    for row in table_data:\n",
    "                        # Join columns by ' | ' (or a comma, etc.)\n",
    "                        table_str += \" | \".join(row) + \"\\n\"\n",
    "                    table_strings.append(table_str)\n",
    "                \n",
    "                tables_doc = Document(\n",
    "                    page_content=\"\\n\\n\".join(table_strings),\n",
    "                    metadata={\"title\": title, \"type\": \"tables\"}\n",
    "                )\n",
    "                documents.append(tables_doc)\n",
    "\n",
    "            # Filter out any complex metadata, if desired\n",
    "            documents = filter_complex_metadata(documents)\n",
    "        else:\n",
    "            print(f\"Warning: No text content found for '{title}'.\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching '{title}': {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../data/qid_to_titles.csv\"\n",
    "df = pd.read_csv(csv_file)  # The file has columns: QID, Wikipedia_Title\n",
    "\n",
    "all_docs = []\n",
    "successful_fetches = 0  # Counter for successful fetches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Wikipedia content: 100%|██████████| 13891/13891 [13:39<00:00, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched 13891 documents out of 13891.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the number of worker threads\n",
    "MAX_WORKERS = 20  # Adjust based on your requirements and Colab's capabilities\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks to the executor\n",
    "    future_to_title = {\n",
    "        executor.submit(fetch_wikipedia_content, row[\"Wikipedia_Title\"]): row[\"Wikipedia_Title\"] \n",
    "        for _, row in df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Use tqdm to display the progress bar\n",
    "    for future in tqdm(as_completed(future_to_title), total=len(future_to_title), desc=\"Fetching Wikipedia content\"):\n",
    "        docs = future.result()\n",
    "        if docs:\n",
    "            all_docs.extend(docs)\n",
    "            successful_fetches += 1\n",
    "\n",
    "print(f\"Successfully fetched {successful_fetches} documents out of {len(df)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved all_docs to ../data/all_docs_final.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_path = '../data/all_docs_final.pkl'\n",
    "\n",
    "# After fetching all_docs\n",
    "with open(pickle_file_path, \"wb\") as f:\n",
    "    pickle.dump(all_docs, f)\n",
    "    print(f\"Successfully saved all_docs to {pickle_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

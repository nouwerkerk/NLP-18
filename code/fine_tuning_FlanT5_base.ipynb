{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o5s8LnJc_zj-"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mj38NJO3_zj_"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# def preprocess_data(data):\n",
        "#     input_texts = [entry['question'] for entry in data]\n",
        "#     target_texts = ['; '.join(entry['final_answers']) for entry in data]\n",
        "#     return input_texts, target_texts\n",
        "\n",
        "def preprocess_data(data):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    for entry in data:\n",
        "        context = f\"Subject: {entry['subject']}, Type: {entry['type']}\"\n",
        "        if 'aliases' in entry and entry['aliases']:\n",
        "            aliases = \", \".join(entry['aliases'])\n",
        "            context += f\", Aliases: {aliases}\"\n",
        "        question = entry['question']\n",
        "        input_texts.append(f\"{context}. {question}\")\n",
        "        target_texts.append('; '.join(entry['final_answers']))\n",
        "    return input_texts, target_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JpQTvEj2_zkA"
      },
      "outputs": [],
      "source": [
        "train_data = load_data('data/train_TLQA.json')\n",
        "test_data = load_data('data/test_TLQA.json')\n",
        "\n",
        "train_input_texts, train_target_texts = preprocess_data(train_data)\n",
        "test_input_texts, test_target_texts = preprocess_data(test_data)\n",
        "\n",
        "def prepare_inference_inputs(data):\n",
        "    input_texts = [entry['question'] for entry in data]\n",
        "    return input_texts\n",
        "\n",
        "# Prepare test inputs without context for inference\n",
        "test_input_texts_without_context = prepare_inference_inputs(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BBsIs45l_zkA"
      },
      "outputs": [],
      "source": [
        "# pip install sentencepiece\n",
        "# pip install pytorch\n",
        "# pip install transformers[torch]\n",
        "\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "max_length = 1024  # Adjust according to your model's capacity\n",
        "train_encodings = tokenizer(train_input_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# train_encodings = tokenizer(train_input_texts, padding=True, return_tensors='pt')\n",
        "train_labels = tokenizer(train_target_texts, padding=True, return_tensors='pt').input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FLXJojsV_zkB"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TLQADataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7kj1NwG_zkC",
        "outputId": "5b045b25-d401-49a1-dec6-cf97639a0135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-15eedb971751>:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "train_dataset = TLQADataset(train_encodings, train_labels)\n",
        "\n",
        "test_encodings = tokenizer(test_input_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "test_labels = tokenizer(test_target_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt').input_ids\n",
        "\n",
        "# Define the evaluation dataset\n",
        "test_dataset = TLQADataset(test_encodings, test_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "vR0Vb5Kk_zkC",
        "outputId": "200aa9a2-e98b-4715-9ae7-ba0b1ac2eab3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250108_153723-p52e0ssw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/roaros-tu-delft/huggingface/runs/p52e0ssw' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/roaros-tu-delft/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/roaros-tu-delft/huggingface' target=\"_blank\">https://wandb.ai/roaros-tu-delft/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/roaros-tu-delft/huggingface/runs/p52e0ssw' target=\"_blank\">https://wandb.ai/roaros-tu-delft/huggingface/runs/p52e0ssw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1283' max='1606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1283/1606 13:04 < 03:17, 1.63 it/s, Epoch 1.60/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.377300</td>\n",
              "      <td>0.320320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg1oXbCz_zkC"
      },
      "outputs": [],
      "source": [
        "def generate_answers(model, tokenizer, inputs):\n",
        "    inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "    outputs = model.generate(inputs.input_ids)\n",
        "    answers = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return answers\n",
        "\n",
        "def generate_answers_2(model, tokenizer, inputs, batch_size=8):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    answers = []\n",
        "\n",
        "    # Process the inputs in batches\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        batch_inputs = inputs[i:i + batch_size]\n",
        "        encoded_inputs = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        encoded_inputs = {key: val.to(model.device) for key, val in encoded_inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**encoded_inputs)\n",
        "\n",
        "        batch_answers = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "        answers.extend(batch_answers)\n",
        "\n",
        "    return answers\n",
        "\n",
        "def generate_answers_3(model, tokenizer, inputs, batch_size=8):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_predictions = []\n",
        "\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        batch_inputs = inputs[i:i + batch_size]\n",
        "        encoding = tokenizer(batch_inputs, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024)\n",
        "        encoding = encoding.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(encoding.input_ids)\n",
        "\n",
        "        predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in output]\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "    return all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye-P_CooCCPZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Exact Match metric\n",
        "def compute_exact_match(predictions, references):\n",
        "    return sum([1 if pred.strip().lower() == ref.strip().lower() else 0 for pred, ref in zip(predictions, references)]) / len(references)\n",
        "\n",
        "# F1 score metric\n",
        "def compute_f1(predictions, references):\n",
        "    def get_tokens(text):\n",
        "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    f1_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "      pred_tokens = get_tokens(pred)\n",
        "      ref_tokens = get_tokens(ref)\n",
        "\n",
        "      common = set(pred_tokens) & set(ref_tokens)\n",
        "      if not common:\n",
        "          f1_scores.append(0)\n",
        "          continue\n",
        "\n",
        "      precision = len(common) / len(pred_tokens)\n",
        "      recall = len(common) / len(ref_tokens)\n",
        "      f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "    return sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "# Time metric\n",
        "def extract_years(text):\n",
        "  return re.findall(r'\\b(19|20)\\d{2}\\b', text)\n",
        "\n",
        "def compute_time_metric(predictions, references):\n",
        "    time_metric_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_years = set(extract_years(pred))\n",
        "        ref_years = set(extract_years(ref))\n",
        "\n",
        "        if not ref_years:\n",
        "            # If there are no years in the reference, consider it perfect (or adjust based on your criteria)\n",
        "            time_metric_scores.append(1.0)\n",
        "            continue\n",
        "\n",
        "        if not pred_years:\n",
        "            # If there are no years in the prediction but there are in the reference, it's incorrect\n",
        "            time_metric_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        intersection = pred_years & ref_years\n",
        "        union = pred_years | ref_years\n",
        "        time_metric_scores.append(len(intersection) / len(union))\n",
        "\n",
        "    return sum(time_metric_scores) / len(time_metric_scores)\n",
        "\n",
        "# Completeness metric\n",
        "def compute_completeness(predictions, references):\n",
        "    def list_contains_all(sublist, mainlist):\n",
        "        return all(item in mainlist for item in sublist)\n",
        "\n",
        "    completeness_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_items = pred.split('; ')\n",
        "        ref_items = ref.split('; ')\n",
        "        completeness_scores.append(list_contains_all(ref_items, pred_items))\n",
        "\n",
        "    return sum(completeness_scores) / len(completeness_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF52uLE6Db5v"
      },
      "outputs": [],
      "source": [
        "def evaluate(predictions, references):\n",
        "    em = compute_exact_match(predictions, references)\n",
        "    f1 = compute_f1(predictions, references)\n",
        "    time_metric = compute_time_metric(predictions, references)\n",
        "    completeness = compute_completeness(predictions, references)\n",
        "\n",
        "    print(f\"Exact Match: {em * 100:.2f}%\")\n",
        "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
        "    print(f\"TimeMetric: {time_metric * 100:.2f}%\")\n",
        "    print(f\"Completeness: {completeness * 100:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"EM\": em,\n",
        "        \"F1\": f1,\n",
        "        \"TimeMetric\": time_metric,\n",
        "        \"Completeness\": completeness\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0UhTPcLEPVE"
      },
      "outputs": [],
      "source": [
        "predictions = generate_answers_2(model, tokenizer, test_input_texts)\n",
        "evaluation_results = evaluate(predictions, test_target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db6PTalkMIFd"
      },
      "outputs": [],
      "source": [
        "def display_test_cases(model, tokenizer, test_inputs, test_targets, num_cases=5):\n",
        "    # Generate answers using the model\n",
        "    predictions = generate_answers_2(model, tokenizer, test_inputs[:num_cases])\n",
        "\n",
        "    # Display each test case with the original question, expected answer, and model prediction\n",
        "    for i in range(num_cases):\n",
        "        print(f\"Test Case {i+1}:\")\n",
        "        print(f\"Question: {test_inputs[i]}\")\n",
        "        print(f\"Expected Answer: {test_targets[i]}\")\n",
        "        print(f\"Model's Answer: {predictions[i]}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# Call the function to display the first 5 test cases\n",
        "display_test_cases(model, tokenizer, test_input_texts, test_target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract questions and answers from the training data for KNN\n",
        "train_questions = [entry['question'] for entry in train_data]\n",
        "train_answers = ['; '.join(entry['final_answers']) for entry in train_data]"
      ],
      "metadata": {
        "id": "gSGjUPpF_HNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnnSearch:\n",
        "    def __init__(self,data=None, num_trees=None,emb_dim=None):\n",
        "        self.num_trees=num_trees\n",
        "        self.emb_dim=emb_dim\n",
        "    def get_embeddings_for_data(self, data_ls):\n",
        "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "        embeddings = model.encode(data_ls)\n",
        "        return embeddings\n",
        "\n",
        "    def get_top_n_neighbours(self,sentence,data_emb,transfer_data,k):\n",
        "        sent_emb = self.get_embeddings_for_data(sentence)\n",
        "        #data_emb = self.get_embeddings_for_data(transfer_questions)\n",
        "        top_questions = []\n",
        "\n",
        "        print(\"new_emb\", sent_emb.shape, data_emb.shape)\n",
        "        text_sims = cosine_similarity(data_emb, [sent_emb]).tolist()\n",
        "        results_sims = zip(range(len(text_sims)), text_sims)\n",
        "        sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        #print(\"text_sims\",sorted_similarities[:2])\n",
        "        for idx, item in sorted_similarities[:k]:\n",
        "                #if item[0] > 0.45:\n",
        "                    top_questions.append(transfer_data[idx])\n",
        "\n",
        "        # text_sims = cosine_similarity(strategy_emb, [sent_emb]).tolist()\n",
        "        # results_sims = zip(range(len(text_sims)), text_sims)\n",
        "        # sorted_similarities = sorted(results_sims, key=lambda x: x[1], reverse=True)\n",
        "        #print(\"text_sims\",sorted_similarities[:2])\n",
        "        # for idx, item in sorted_similarities:\n",
        "        #         top_questions.append(str_qa[idx])\n",
        "        return top_questions"
      ],
      "metadata": {
        "id": "ws2U7BmU_KUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Instantiate KnnSearch\n",
        "knn_search = KnnSearch()\n",
        "\n",
        "# Get embeddings for training data\n",
        "train_embeddings = knn_search.get_embeddings_for_data(train_questions)\n",
        "print(train_embeddings)"
      ],
      "metadata": {
        "id": "6UgEmTWL_TJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_few_shot_examples(test_question, k=5):\n",
        "    combined_data = [{'question': q, 'answer': a} for q, a in zip(train_questions, train_answers)]\n",
        "\n",
        "    # Get the top-k similar questions\n",
        "    few_shot_examples = knn_search.get_top_n_neighbours(\n",
        "        sentence=test_question,\n",
        "        data_emb=train_embeddings,\n",
        "        transfer_data=combined_data,\n",
        "        k=k\n",
        "    )\n",
        "    return few_shot_examples\n",
        "\n",
        "def create_few_shot_prompt(few_shot_examples, test_question):\n",
        "    prompt = \"\"\n",
        "    for example in few_shot_examples:\n",
        "        prompt += f\"Q: {example['question']}\\nA: {example['answer']}\\n\\n\"\n",
        "    prompt += f\"Q: {test_question}\\nA:\"\n",
        "    return prompt\n",
        "\n",
        "def generate_few_shot_predictions(model, tokenizer, test_questions, k=5):\n",
        "    predictions = []\n",
        "    for test_question in test_questions:\n",
        "        few_shot_examples = get_few_shot_examples(test_question, k)\n",
        "        prompt = create_few_shot_prompt(few_shot_examples, test_question)\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        # outputs = model.generate(input_ids, max_length=512)\n",
        "        # prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # predictions.append(prediction)\n",
        "    # return predictions\n",
        "    return []"
      ],
      "metadata": {
        "id": "Ync_TjKo_Wnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "\n",
        "\n",
        "test_question = \"List all sports teams Andy Carroll played for from 2010 to 2020.\"\n",
        "\n",
        "# Pass the test question as a single-element list\n",
        "few_shot_examples = knn_search.get_top_n_neighbours(\n",
        "    sentence=test_question,  # Wrap the test question in a list\n",
        "    data_emb=train_embeddings,  # Ensure this is 2D\n",
        "    transfer_data=train_questions,  # The list of training questions\n",
        "    k=5  # Number of nearest neighbors to retrieve\n",
        ")\n",
        "print(few_shot_examples)\n",
        "\n",
        "few_shot_predictions = generate_few_shot_predictions(model, tokenizer, test_input_texts, k=5)\n",
        "evaluation_results_few_shot = evaluate(few_shot_predictions, test_target_texts)"
      ],
      "metadata": {
        "id": "6Jg11Njg_cHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
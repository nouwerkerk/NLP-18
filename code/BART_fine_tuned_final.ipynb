{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k4Bz8mLwHeTc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(data):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    for entry in data:\n",
        "        context = f\"Subject: {entry['subject']}, Type: {entry['type']}\"\n",
        "        if 'aliases' in entry and entry['aliases']:\n",
        "            aliases = \", \".join(entry['aliases'])\n",
        "            context += f\", Aliases: {aliases}\"\n",
        "        question = entry['question']\n",
        "        input_texts.append(f\"{question}\")\n",
        "        target_texts.append('; '.join(entry['final_answers']))\n",
        "    return input_texts, target_texts\n",
        "\n",
        "train_data = load_data('data/train_TLQA.json')\n",
        "test_data = load_data('data/test_TLQA.json')\n",
        "\n",
        "train_input_texts, train_target_texts = preprocess_data(train_data)\n",
        "test_input_texts, test_target_texts = preprocess_data(test_data)"
      ],
      "metadata": {
        "id": "KrzUZ1X5HoBK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TLQADataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item"
      ],
      "metadata": {
        "id": "stUJzwtwHoRx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and encoding\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "max_length = 1024\n",
        "train_encodings = tokenizer(train_input_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "train_labels = tokenizer(train_target_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt').input_ids\n",
        "train_dataset = TLQADataset(train_encodings, train_labels)"
      ],
      "metadata": {
        "id": "ZFxziLxLHtWr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_bart\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs_bart\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlTlYvraHydc",
        "outputId": "65ce81fc-e126-446a-9cbd-90c46d519aa8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-21-ecee070f17bf>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "67-ozczdH1C8",
        "outputId": "1549ca32-0e0f-42bf-abb8-248babad454e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1606' max='1606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1606/1606 19:47, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>16.974100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>15.209400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>13.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>12.381200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>11.446100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>10.791400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>10.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>9.385300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>8.715700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>7.783900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>6.725500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>6.088900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>5.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>5.109600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>4.480100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>4.111800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.742800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>3.310800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.880300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.413100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.523100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.167500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.791300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.517200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.494600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.399500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.382200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.372300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.418900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.392600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.266000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.321100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.342600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.361300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.285000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.307200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.313900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.331900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.341400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.297700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.304600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.389700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.309800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.317800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.271700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.325800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.261000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.286000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.290900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.356600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.279600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.307700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.421300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.304100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.288500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.320200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.262000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.256900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.327600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.310500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.315800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.352900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.295800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.294900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.272600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.342500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.292800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.270500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.264400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.250600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.246800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.314700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.223200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.212900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.249300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.256400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.234200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.242200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.207200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.219400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.317800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.280700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.222900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.202400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.255300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.209500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.248100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.204500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.285300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.201800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.276100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.242100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.214300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.269100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.171600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.193500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.203500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.206400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.199400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.201200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.233300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.280100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.234200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>0.222800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.235100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>0.195600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.293200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.176100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>0.230300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.235800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.180900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.297500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.171800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.245300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.171800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>0.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>0.192400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>0.194300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>0.201300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>0.216700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>0.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>0.163300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.232100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>0.196800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>0.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>0.204400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>0.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>0.194200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>0.301300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>0.193100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>0.250400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.174800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1606, training_loss=1.2933066745623853, metrics={'train_runtime': 1187.8816, 'train_samples_per_second': 5.408, 'train_steps_per_second': 1.352, 'total_flos': 1250757965512704.0, 'train_loss': 1.2933066745623853, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./fine_tuned_bart\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKBybu01H2sc",
        "outputId": "0c1733e7-a560-4899-c52a-f71afd2ea712"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_bart/tokenizer_config.json',\n",
              " './fine_tuned_bart/special_tokens_map.json',\n",
              " './fine_tuned_bart/vocab.json',\n",
              " './fine_tuned_bart/merges.txt',\n",
              " './fine_tuned_bart/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KnnSearch:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "    def get_embeddings_for_data(self, data_ls):\n",
        "        return self.model.encode(data_ls)\n",
        "\n",
        "    def get_top_n_neighbours(self, sentence, data_emb, transfer_data, k):\n",
        "        sent_emb = self.get_embeddings_for_data(sentence)\n",
        "        text_sims = cosine_similarity(data_emb, [sent_emb]).flatten()\n",
        "        sorted_indices = text_sims.argsort()[::-1][:k]\n",
        "        return [transfer_data[idx] for idx in sorted_indices]\n",
        "\n",
        "knn_search = KnnSearch()\n",
        "train_embeddings = knn_search.get_embeddings_for_data(train_input_texts)\n",
        "\n",
        "def get_few_shot_examples(test_question, k=5):\n",
        "    combined_data = [{'question': q, 'answer': a} for q, a in zip(train_input_texts, train_target_texts)]\n",
        "    return knn_search.get_top_n_neighbours(test_question, train_embeddings, combined_data, k)\n",
        "\n",
        "def create_few_shot_prompt(few_shot_examples, test_question):\n",
        "    prompt = \"\"\n",
        "    for example in few_shot_examples:\n",
        "        prompt += f\"Q: {example['question']}\\nA: {example['answer']}\\n\\n\"\n",
        "    prompt += f\"Q: {test_question}\\nA:\"\n",
        "    # Truncate to ensure the prompt does not exceed max_length\n",
        "    encoded_prompt = tokenizer(prompt, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    return tokenizer.decode(encoded_prompt['input_ids'][0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "DBx8IU8OH5E9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl = pipeline(\"text2text-generation\", model=\"./fine_tuned_bart\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hO89YmlH_Of",
        "outputId": "13e28425-7a74-4cae-bcfe-7865ead1db9e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_few_shot_predictions_with_pipeline(pipeline, test_questions, k=5, batch_size=8):\n",
        "    predictions = []\n",
        "    for i in range(0, len(test_questions), batch_size):\n",
        "        batch_questions = test_questions[i:i + batch_size]\n",
        "        batch_prompts = []\n",
        "        for test_question in batch_questions:\n",
        "            few_shot_examples = get_few_shot_examples(test_question, k)\n",
        "            prompt = create_few_shot_prompt(few_shot_examples, test_question)\n",
        "            batch_prompts.append(prompt)\n",
        "        batch_predictions = pipeline(batch_prompts, max_length=512, num_return_sequences=1)\n",
        "        predictions.extend([pred[\"generated_text\"] for pred in batch_predictions])\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "q3-7cD1ZIAon"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_predictions = generate_few_shot_predictions_with_pipeline(pl, test_input_texts, k=10)"
      ],
      "metadata": {
        "id": "6uo9FXdeIDwI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_exact_match(predictions, references):\n",
        "    return sum([1 if pred.strip().lower() == ref.strip().lower() else 0 for pred, ref in zip(predictions, references)]) / len(references)\n",
        "\n",
        "def compute_f1(predictions, references):\n",
        "    def get_tokens(text):\n",
        "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    f1_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_tokens = get_tokens(pred)\n",
        "        ref_tokens = get_tokens(ref)\n",
        "        common = set(pred_tokens) & set(ref_tokens)\n",
        "        if not common:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        precision = len(common) / len(pred_tokens)\n",
        "        recall = len(common) / len(ref_tokens)\n",
        "        f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "    return sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "def compute_time_metric(predictions, references):\n",
        "    def extract_years(text):\n",
        "        return re.findall(r'\\b(19|20)\\d{2}\\b', text)\n",
        "    time_metric_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_years = set(extract_years(pred))\n",
        "        ref_years = set(extract_years(ref))\n",
        "        if not ref_years:\n",
        "            time_metric_scores.append(1.0)\n",
        "            continue\n",
        "        if not pred_years:\n",
        "            time_metric_scores.append(0.0)\n",
        "            continue\n",
        "        intersection = pred_years & ref_years\n",
        "        union = pred_years | ref_years\n",
        "        time_metric_scores.append(len(intersection) / len(union))\n",
        "    return sum(time_metric_scores) / len(time_metric_scores)\n",
        "\n",
        "def compute_completeness(predictions, references):\n",
        "    def list_contains_all(sublist, mainlist):\n",
        "        return all(item in mainlist for item in sublist)\n",
        "    completeness_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_items = pred.split('; ')\n",
        "        ref_items = ref.split('; ')\n",
        "        completeness_scores.append(list_contains_all(ref_items, pred_items))\n",
        "    return sum(completeness_scores) / len(completeness_scores)\n",
        "\n",
        "def evaluate(predictions, references):\n",
        "    em = compute_exact_match(predictions, references)\n",
        "    f1 = compute_f1(predictions, references)\n",
        "    time_metric = compute_time_metric(predictions, references)\n",
        "    completeness = compute_completeness(predictions, references)\n",
        "    print(f\"Exact Match: {em * 100:.2f}%\")\n",
        "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
        "    print(f\"TimeMetric: {time_metric * 100:.2f}%\")\n",
        "    print(f\"Completeness: {completeness * 100:.2f}%\")\n",
        "    return {\"EM\": em, \"F1\": f1, \"TimeMetric\": time_metric, \"Completeness\": completeness}"
      ],
      "metadata": {
        "id": "GZwfCPC4IFR5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "evaluation_results_few_shot = evaluate(few_shot_predictions, test_target_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ0i5pgxIHZB",
        "outputId": "d1de1795-9e45-4794-ac52-c729361cd002"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match: 0.00%\n",
            "F1 Score: 39.66%\n",
            "TimeMetric: 99.67%\n",
            "Completeness: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100,105):\n",
        "    print(f\"Test Case {i+1}:\")\n",
        "    print(f\"Question: {test_input_texts[i]}\")\n",
        "    print(f\"Expected Answer: {test_target_texts[i]}\")\n",
        "    print(f\"Model's Answer: {few_shot_predictions[i]}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ysxRMifIYKt",
        "outputId": "4d0ea645-2504-49d1-bedd-be15b877b322"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 101:\n",
            "Question: List all employers Michael Hout, also known as Mike Hout, worked for from 2010 to 2020.\n",
            "Expected Answer: University of California, Berkeley (2010, 2011, 2012, 2013); New York University (2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)\n",
            "Model's Answer: University of Maryland, Baltimore County (2010, 2011, 2012, 2013, 2014); University of Pennsylvania (2014, 2015, 2016)\n",
            "================================================================================\n",
            "Test Case 102:\n",
            "Question: List all heads of the government of Guinea, also known as Guinea-Conakry, from 2010 to 2020 \n",
            "Expected Answer: Mamady Youla (2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018); Ibrahima Kassory Fofana (2018, 2019, 2020)\n",
            "Model's Answer: Jean-Claude Boubacar√© (2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020); Abdoulaye N'Diaye (2019, 2020)\n",
            "================================================================================\n",
            "Test Case 103:\n",
            "Question: List all employers Linda Bauld, also known as Linda C. Bauld, worked for from 2014 to 2020.\n",
            "Expected Answer: University of Stirling (2014, 2015, 2016, 2017, 2018); University of Edinburgh (2018, 2019, 2020)\n",
            "Model's Answer: University of California, Berkeley (2010, 2011, 2012, 2013, 2014, 2015, 2016); University of Texas MD Anderson Cancer Center (2016, 2017)\n",
            "================================================================================\n",
            "Test Case 104:\n",
            "Question: List all coaches of PSV Eindhoven, also known as Philips Sport Vereniging, from 2013 to 2020\n",
            "Expected Answer: Phillip Cocu (2013, 2014, 2015, 2016, 2017, 2018); Mark van Bommel (2018, 2019); Ernest Faber (2019, 2020); Roger Schmidt (2020)\n",
            "Model's Answer: Nicolae Mihailoviƒá (2011, 2012, 2013, 2014, 2015, 2016, 2017); Dan Petrescu (2017, 2018, 2019, 2020)\n",
            "================================================================================\n",
            "Test Case 105:\n",
            "Question: List all employers Pedro J. Ram√≠rez worked for from 2010 to 2020.\n",
            "Expected Answer: El Mundo (2010, 2011, 2012, 2013, 2014); El Espa√±ol (2015, 2016, 2017, 2018, 2019, 2020)\n",
            "Model's Answer: University of California, Berkeley (2010, 2011, 2012, 2013, 2014, 2015); University of Texas System (2015, 2016)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Assuming `few_shot_predictions` is your list of predictions and `test_input_texts` contains the questions.\n",
        "\n",
        "# Format the predictions\n",
        "output_data = []\n",
        "for i, prediction in enumerate(few_shot_predictions):\n",
        "    entry = {\n",
        "        \"question\": test_input_texts[i],\n",
        "        \"generated_answer\": [prediction.strip()],\n",
        "        \"ground_truth\": [test_target_texts[i]]\n",
        "    }\n",
        "    output_data.append(entry)\n",
        "\n",
        "# Write to a JSON file\n",
        "output_file_path = \"BART_predictions_k_10.json\"\n",
        "with open(output_file_path, \"w\") as outfile:\n",
        "    json.dump(output_data, outfile, indent=4)\n",
        "\n",
        "print(f\"Predictions saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYXEIFRnIQnT",
        "outputId": "2fbb405c-f5d8-4c72-e561-b229ca7ea5eb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to BART_predictions_k_10.json\n"
          ]
        }
      ]
    }
  ]
}
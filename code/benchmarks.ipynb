{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "tiCQyHAAhYwa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "#input and output file names\n",
        "JSON_FILE_NAME = \"FLanT5_predictions_k_3_processed\"\n",
        "OUTPUT_FILE_NAME = \"test3_benchmarks\"\n",
        "\n",
        "#load json file\n",
        "f = open(f'{JSON_FILE_NAME}.json')\n",
        "output = json.load(f)\n",
        "\n",
        "#print(json.dumps(output, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "def preprocessing(json_data):\n",
        "\n",
        "  def parse_entry(entry):\n",
        "    # Regular expression to capture the entity and years\n",
        "    match = re.match(r\"(.+?)\\s*\\(([\\d,\\s\\u2013\\-]*)\\)?\", entry)\n",
        "    if match:\n",
        "        entity = match.group(1).strip()  # Extract the entity name\n",
        "        timeline = match.group(2).strip() if match.group(2) else \"\"  # Extract the timeline if present\n",
        "        if timeline:\n",
        "            # Attempt to split timeline into valid integers, ignoring invalid entries\n",
        "            years = []\n",
        "            for part in timeline.split(\",\"):\n",
        "\n",
        "                try:\n",
        "                    # Support ranges (e.g., \"2010–2017\")\n",
        "                    if \"–\" in part:\n",
        "                        start, end = map(int, part.split(\"–\"))\n",
        "                        years.extend(range(start, end + 1))\n",
        "                    else:\n",
        "                        years.append(int(part))\n",
        "                except ValueError:\n",
        "                    pass  # Skip invalid entries\n",
        "            return {entity: years}\n",
        "        else:\n",
        "            return {entity: []}  # No timeline or invalid timeline\n",
        "    else:\n",
        "        # If the pattern doesn't match, assume the entire entry is an entity with no timeline\n",
        "        return {entry.strip(): []}\n",
        "\n",
        "  json_data_copy = copy.deepcopy(json_data)\n",
        "\n",
        "  for x in json_data_copy:\n",
        "    x[\"generated_answer\"] = list(dict.fromkeys(x[\"generated_answer\"])) # remove duplicates\n",
        "    parsed_generated_answer = [parse_entry(entry) for entry in x[\"generated_answer\"]]\n",
        "    parsed_ground_truth = [parse_entry(entry) for entry in x[\"ground_truth\"]]\n",
        "\n",
        "    x[\"generated_answer\"] = parsed_generated_answer\n",
        "    x[\"ground_truth\"] = parsed_ground_truth\n",
        "\n",
        "  return json_data_copy"
      ],
      "metadata": {
        "id": "9wXgDaVkgWX5"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entity-level evaluation\n",
        "\n",
        "def entity_precision_recall_f1_EM(data):\n",
        "  total_correct_generated = 0\n",
        "  total_generated = 0\n",
        "  total_ground_truth = 0\n",
        "  exact_match = 0\n",
        "  total = 0\n",
        "\n",
        "  for entry in data:\n",
        "\n",
        "      generated_entities = {list(d.keys())[0] for d in entry[\"generated_answer\"]}\n",
        "      ground_truth_entities = {list(d.keys())[0] for d in entry[\"ground_truth\"]}\n",
        "      correct_entities = generated_entities.intersection(ground_truth_entities)\n",
        "\n",
        "      total_correct_generated += len(correct_entities)\n",
        "      total_generated += len(generated_entities)\n",
        "      total_ground_truth += len(ground_truth_entities)\n",
        "\n",
        "      if generated_entities == ground_truth_entities:\n",
        "        exact_match += 1\n",
        "\n",
        "      total += 1\n",
        "\n",
        "  precision = total_correct_generated / total_generated\n",
        "  recall = total_correct_generated / total_ground_truth\n",
        "  f1 = 0.0\n",
        "  if (precision > 0.0 or recall > 0.0):\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "  EM = (exact_match / total)\n",
        "\n",
        "  return precision, recall, f1, EM"
      ],
      "metadata": {
        "id": "MTThZtccmbeO"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Timeline-level evaluation\n",
        "\n",
        "def timeline_precision_recall_f1(data):\n",
        "  total_correct_generated = 0\n",
        "  total_generated = 0\n",
        "  total_ground_truth = 0\n",
        "\n",
        "  for entry in data:\n",
        "\n",
        "      #print(entry)\n",
        "      generated_years = [list(d.values())[0] for d in entry[\"generated_answer\"]]\n",
        "      generated_years  = [year for sublist in generated_years for year in sublist]\n",
        "\n",
        "      ground_truth_years = [list(d.values())[0] for d in entry[\"ground_truth\"]]\n",
        "      ground_truth_years  = [year for sublist in ground_truth_years for year in sublist]\n",
        "\n",
        "      def match_lists(list1, list2):\n",
        "        match_count = 0\n",
        "        list2_copy = list2.copy()\n",
        "\n",
        "        for item in list1:\n",
        "            if item in list2_copy:\n",
        "                match_count += 1\n",
        "                list2_copy.remove(item)\n",
        "        return match_count\n",
        "\n",
        "      correct_years = match_lists(generated_years, ground_truth_years)\n",
        "\n",
        "      total_correct_generated += correct_years\n",
        "      total_generated += len(generated_years)\n",
        "      total_ground_truth += len(ground_truth_years)\n",
        "\n",
        "  precision = total_correct_generated / total_generated\n",
        "  recall = total_correct_generated / total_ground_truth\n",
        "  f1 = 0.0\n",
        "  if (precision > 0.0 or recall > 0.0):\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  return precision, recall, f1\n",
        "\n",
        "def timeline_EM(data):\n",
        "\n",
        "  def check_timeline_EM(generated_timelines, ground_truth_timelines):\n",
        "    # Sort the inner lists\n",
        "    sorted_list1 = [sorted(inner_list) for inner_list in generated_timelines]\n",
        "    sorted_list2 = [sorted(inner_list) for inner_list in ground_truth_timelines]\n",
        "\n",
        "    # Sort the outer lists and compare\n",
        "    return sorted(sorted_list1) == sorted(sorted_list2)\n",
        "\n",
        "  exact_match = 0\n",
        "  total = 0\n",
        "\n",
        "  for entry in data:\n",
        "    generated_years = [list(d.values())[0] for d in entry[\"generated_answer\"]]\n",
        "    #print(generated_years)\n",
        "\n",
        "    ground_truth_years = [list(d.values())[0] for d in entry[\"ground_truth\"]]\n",
        "\n",
        "    if check_timeline_EM(generated_years, ground_truth_years):\n",
        "      exact_match += 1\n",
        "\n",
        "    total += 1\n",
        "\n",
        "  EM = (exact_match / total)\n",
        "  return EM"
      ],
      "metadata": {
        "id": "DmdHC7ZOGcdb"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change list of dictionaries into one dictionary for comparisons\n",
        "def combine_generated_answers(data):\n",
        "    data_copy = copy.deepcopy(data)\n",
        "    combined = {}\n",
        "\n",
        "    # Iterate over each dictionary in the list\n",
        "    for item in data_copy[\"generated_answer\"]:\n",
        "        for key, value in item.items():\n",
        "            if key in combined:\n",
        "                # Merge and remove duplicates\n",
        "                combined[key] = sorted(set(combined[key] + value))\n",
        "            else:\n",
        "                combined[key] = value\n",
        "\n",
        "    # Replace the list of dictionaries with the combined result\n",
        "    data_copy[\"generated_answer\"] = [{k: v} for k, v in combined.items()]\n",
        "    return data_copy"
      ],
      "metadata": {
        "id": "3pGR6xTmA04m"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined evaluation\n",
        "\n",
        "def combined_precision_recall_f1_EM(data):\n",
        "  total_correct_generated = 0\n",
        "  total_generated = 0\n",
        "  total_ground_truth = 0\n",
        "  exact_match = 0\n",
        "  total = 0\n",
        "\n",
        "  for entry in data:\n",
        "    # change list of dictionaries into one dictionary for comparisons\n",
        "    generated_answer_dict = {}\n",
        "    ground_truth_dict = {}\n",
        "\n",
        "    for d in entry['generated_answer']:\n",
        "      generated_answer_dict.update(d)\n",
        "\n",
        "    for d in entry['ground_truth']:\n",
        "      ground_truth_dict.update(d)\n",
        "\n",
        "    correct_generated = sum(1 for k, v in generated_answer_dict.items() if ground_truth_dict.get(k) == v)\n",
        "\n",
        "    total_correct_generated += correct_generated\n",
        "    total_generated += len(generated_answer_dict)\n",
        "    total_ground_truth += len(ground_truth_dict)\n",
        "\n",
        "    if (generated_answer_dict == ground_truth_dict):\n",
        "      exact_match += 1\n",
        "    total += 1\n",
        "\n",
        "  precision = total_correct_generated / total_generated\n",
        "  recall = total_correct_generated / total_ground_truth\n",
        "  f1 = 0.0\n",
        "  if (precision > 0.0 or recall > 0.0):\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "  EM = (exact_match / total)\n",
        "\n",
        "  return precision, recall, f1, EM"
      ],
      "metadata": {
        "id": "4tCROaONRfyA"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocessing(output)\n",
        "#print(json.dumps(preprocessed_data, indent=4))"
      ],
      "metadata": {
        "id": "aBOcnz7jXJ5z"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print all fully correct answers\n",
        "\n",
        "#for entry in preprocessed_data:\n",
        "#  if entry['generated_answer'] == entry['ground_truth']:\n",
        "#    print(entry)"
      ],
      "metadata": {
        "id": "IJXAdfy7rjav"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_results = entity_precision_recall_f1_EM(preprocessed_data)\n",
        "timeline_results_pre_rec_f1 = timeline_precision_recall_f1(preprocessed_data)\n",
        "timeline_results_EM = timeline_EM(preprocessed_data)\n",
        "\n",
        "new_preprocessed_data = [combine_generated_answers(entry) for entry in preprocessed_data]\n",
        "combined_results = combined_precision_recall_f1_EM(new_preprocessed_data)"
      ],
      "metadata": {
        "id": "-hToMsfBdBfo"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_string =  \"########################\\n\"\n",
        "result_string += \"Entity evaluation\\n\"\n",
        "result_string += \"########################\\n\"\n",
        "\n",
        "result_string += f\"Precision:\\t\\t {entity_results[0]}\\n\"\n",
        "result_string += f\"Recall (completeness):\\t {entity_results[1]}\\n\"\n",
        "result_string += f\"f1:\\t\\t\\t {entity_results[2]}\\n\"\n",
        "result_string += f\"EM:\\t\\t\\t {entity_results[3]}\\n\\n\"\n",
        "\n",
        "result_string += \"########################\\n\"\n",
        "result_string += \"Timeline evaluation\\n\"\n",
        "result_string += \"########################\\n\"\n",
        "\n",
        "result_string += f\"Precision:\\t\\t {timeline_results_pre_rec_f1[0]}\\n\"\n",
        "result_string += f\"Recall (completeness):\\t {timeline_results_pre_rec_f1[1]}\\n\"\n",
        "result_string += f\"f1:\\t\\t\\t {timeline_results_pre_rec_f1[2]}\\n\"\n",
        "result_string += f\"EM:\\t\\t\\t {timeline_results_EM}\\n\\n\"\n",
        "\n",
        "result_string += \"########################\\n\"\n",
        "result_string += \"Combined evaluation\\n\"\n",
        "result_string += \"########################\\n\"\n",
        "\n",
        "result_string += f\"Precision:\\t\\t {combined_results[0]}\\n\"\n",
        "result_string += f\"Recall (completeness):\\t {combined_results[1]}\\n\"\n",
        "result_string += f\"f1:\\t\\t\\t {combined_results[2]}\\n\"\n",
        "result_string += f\"EM:\\t\\t\\t {combined_results[3]}\\n\\n\"\n",
        "\n",
        "print(result_string)\n",
        "\n",
        "f = open(f\"{OUTPUT_FILE_NAME}.txt\", \"w\")\n",
        "f.write(result_string)\n",
        "f.close()\n",
        "\n",
        "print(f'Results saved to {OUTPUT_FILE_NAME}.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IEWXsFmccGG",
        "outputId": "45093242-b9d7-4cd1-df9e-5a2352d01093"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "Entity evaluation\n",
            "########################\n",
            "Precision:\t\t 0.05070842654735272\n",
            "Recall (completeness):\t 0.021518987341772152\n",
            "f1:\t\t\t 0.03021550766496334\n",
            "EM:\t\t\t 0.0009337068160597573\n",
            "\n",
            "########################\n",
            "Timeline evaluation\n",
            "########################\n",
            "Precision:\t\t 0.7835351089588378\n",
            "Recall (completeness):\t 0.8061783756851021\n",
            "f1:\t\t\t 0.7946954813359528\n",
            "EM:\t\t\t 0.022408963585434174\n",
            "\n",
            "########################\n",
            "Combined evaluation\n",
            "########################\n",
            "Precision:\t\t 0.005965697240865026\n",
            "Recall (completeness):\t 0.002531645569620253\n",
            "f1:\t\t\t 0.0035547656076427457\n",
            "EM:\t\t\t 0.0\n",
            "\n",
            "\n",
            "Results saved to test3_benchmarks.txt\n"
          ]
        }
      ]
    }
  ]
}